{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-76e00caa7ec0>, line 113)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-76e00caa7ec0>\"\u001b[0;36m, line \u001b[0;32m113\u001b[0m\n\u001b[0;31m    def test(testData,tree):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import array\n",
    "\n",
    "# NOTE: This program does not work! Instead, I've commented what I intended to do with each function.\n",
    "# List of functions: entropy, infoGain, ID3, predict, and testModel\n",
    "\n",
    "# IMPLEMENTATION:\n",
    "# b) four features I could have extracted: Instead of just extracted the first name and last name\n",
    "#    from the examples, I could have extracted (length of first name), (length of both names),\n",
    "#   (name includes punctuation?), or (name contains how many values). There are any number of features\n",
    "#   that could be instructive to the decision tree and the output that could be obtained.\n",
    "# c/d) could not report test or training error on the data set. See pseudocode below.\n",
    "# ----------------------------------------\n",
    "# LIMITING DEPTH: could not perform limiting depth \n",
    "\n",
    "\n",
    "\n",
    "#import data from training.data in, features are delimited by ' ' (label, first name, last name)\n",
    "dataset = pd.read_table('training.data', delimiter=' ', names=('label', 'first name', 'last name'))\n",
    "\n",
    "\n",
    "def entropy(featureVector):\n",
    "    # calcualtes the uncertainty of classifying an example based on some input featureVector\n",
    "    elements,counts = np.unique(featureVector,return_count = True)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy\n",
    "\n",
    "def InformationGain(examples, selectedAttrib, targetName =\"class\"):\n",
    "    # calculate IG for the examples. Takes three parameters: examples (dataset for the feature at which IG\n",
    "    # should be calculated), selectedAttrib (name of the feature to IG should be calculated on),\n",
    "    # and targetName (name of label)\n",
    "    \n",
    "    labelEntropy = entropy(examples[targetName])\n",
    "    vals, counts = np.unique(examples[selectedAttrib], return_counts=True)\n",
    "    \n",
    "    # feature entropy: entropy of individual values of measured attributes in the dataset.\n",
    "    # the sum of each value that an attribute A can take, over the proportion of each (+) or (-) label\n",
    "    # outcome related to the value of the attribute. computes the entropy for one attribute value\n",
    "    featureEntropy = np.sum([(counts[i]/np.sum(counts))\n",
    "                               *entropy(examples.where(examples[selectedAttrib]==vals[i])) \n",
    "                               for i in range(len(vals))])\n",
    "    \n",
    "    # IG for the model: entropy of all the labels subtracted by the entropy of each feature\n",
    "    # (sum of all feature entropy*proportion of feature occurence in the example set)\n",
    "    InfoGain = labelEntropy - featureEntropy\n",
    "    return InfoGain\n",
    "\n",
    "def ID3(examples,orig_data,features,labelName=\"class\",parentNode=None):\n",
    "    # if each example shares the same label, return one node with that label \n",
    "    if len(np.unique(examples[labelName])) <= 1:\n",
    "        return np.unique(examples[labelName])[0]\n",
    "    \n",
    "    # if number of examples is 0, return the parent node    \n",
    "    elif len(examples)==0:\n",
    "        return parentNode\n",
    "    \n",
    "    # else, find the best feature to split the data on by calling InformationGain\n",
    "    # let the tree get the pureFeature, for every value v that the examples can assume \n",
    "    # with some pureFeature, create a branch with that value. if that branch is empty,\n",
    "    # put common label under branch. Else, make a recursive to ID3 on the subset of examples with\n",
    "    # value v set to feature. Return the tree\n",
    "    else:\n",
    "        parentNode = np.unique(examples[labelName])\n",
    "        [np.argmax(np.unique(examples[labelName],return_counts=True)[1])]\n",
    "        \n",
    "        featureValues=[InformationGain(examples,feature,labelName) for feature in features]\n",
    "        pureFeatureIndex = np.argmax(featureValues)\n",
    "        pureFeature = features[pureFeatureIndex]\n",
    "        \n",
    "        # create tree split on purest feature (feature that best classifies the set of examples)\n",
    "        # update the set of features to not include purefeature \n",
    "        tree = {pureFeature:{}}\n",
    "        features = [i for i in features if i != pureFeature]\n",
    "        \n",
    "        for value in np.unique(data[pureFeature]):\n",
    "            value = value\n",
    "            # subExamples gets the subset of examples where the attribute (measured feature) is set \n",
    "            # to a certain value\n",
    "            subExamples = examples.where(examples[pureFeature] == value).dropna()\n",
    "            subtree = ID3(subExamples,dataset,features,labelName,parentNode)\n",
    "            # create subtree (recursive call), add to tree\n",
    "            tree[pureFeature][value] = subtree\n",
    "            \n",
    "        return(tree)\n",
    "\n",
    "# (predict function based on online decision model)\n",
    "# checks the tree based on prediction and query\n",
    "# returns correct prediction if correct, else\n",
    "# returns the wrong result for a test example\n",
    "    \n",
    "def predict(query,tree,default = 1):\n",
    "    for key in list(query.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "            \n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "                result = tree[key][query[key]]\n",
    "        \n",
    "            if isinstance(result,dict):\n",
    "                return predict(query,result)\n",
    "            else:\n",
    "                return result\n",
    "           \n",
    "            \n",
    "def trainTree(dataset):\n",
    "    # The training function for the dataset. \n",
    "   \n",
    "\n",
    "def test(testData,tree):   \n",
    "   \n",
    "    # in test, I want to open the test set and, for the length of the data, call the predict function\n",
    "    # on the tree produced by ID3. Then the prediction accuracy would be calculated by comparing\n",
    "    # the accuracy of predicted to actual label value of the test data, divided by length of the data\n",
    "    for i in range(len(testData)):\n",
    "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n",
    "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == testData[\"class\"])/len(testData))*100,'%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
